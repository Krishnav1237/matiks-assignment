<?xml version="1.0" encoding="UTF-8"?>
<project_specification>
  <metadata>
    <project_name>Matiks Social Media Monitoring & Review Aggregation Tool</project_name>
    <version>1.0</version>
    <created_date>2026-01-31</created_date>
  </metadata>

  <executive_summary>
    <objective>Build a fully automated, self-sustaining system that monitors brand mentions across social platforms and aggregates app store reviews without using official APIs for X/Twitter and LinkedIn</objective>
    <key_constraint>Scrapers must be production-grade with anti-detection, rate limiting, proxy rotation, and resilience against blocking</key_constraint>
  </executive_summary>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 1: ARCHITECTURE OVERVIEW -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <architecture>
    <overview>
      <description>Microservices-based architecture with independent scrapers, centralized data storage, and a web dashboard</description>
      <deployment>Docker containers orchestrated via Docker Compose, deployable to any VPS or cloud platform</deployment>
    </overview>

    <components>
      <component name="Scraper Engine">
        <purpose>Core scraping infrastructure with browser automation and anti-detection</purpose>
        <technology>Playwright with stealth plugins, rotating residential proxies</technology>
      </component>
      
      <component name="Platform Scrapers">
        <purpose>Individual scraper modules for each platform</purpose>
        <platforms>Reddit, X/Twitter, LinkedIn, Google Play Store, Apple App Store</platforms>
      </component>
      
      <component name="Scheduler">
        <purpose>Orchestrates scraping jobs with configurable intervals</purpose>
        <technology>Node-cron or APScheduler with job queue (Bull/Celery)</technology>
      </component>
      
      <component name="Data Pipeline">
        <purpose>Processes, normalizes, and enriches scraped data</purpose>
        <features>Deduplication, sentiment analysis, entity extraction</features>
      </component>
      
      <component name="Database">
        <purpose>Persistent storage for all collected data</purpose>
        <technology>PostgreSQL with TimescaleDB extension for time-series optimization</technology>
      </component>
      
      <component name="Web Dashboard">
        <purpose>User interface for viewing and filtering data</purpose>
        <technology>Next.js with Tailwind CSS</technology>
      </component>
      
      <component name="Export Service">
        <purpose>Automated spreadsheet generation and cloud sync</purpose>
        <technology>Google Sheets API integration</technology>
      </component>
    </components>

    <directory_structure>
      <![CDATA[
matiks-monitor/
├── docker-compose.yml
├── .env.example
├── README.md
├── docs/
│   ├── SETUP.md
│   ├── ARCHITECTURE.md
│   └── TROUBLESHOOTING.md
│
├── packages/
│   ├── core/                    # Shared utilities
│   │   ├── src/
│   │   │   ├── browser/         # Browser automation
│   │   │   │   ├── BrowserPool.ts
│   │   │   │   ├── StealthConfig.ts
│   │   │   │   └── ProxyManager.ts
│   │   │   ├── antidetect/      # Anti-detection measures
│   │   │   │   ├── fingerprint.ts
│   │   │   │   ├── humanBehavior.ts
│   │   │   │   └── captchaSolver.ts
│   │   │   ├── queue/           # Job queue
│   │   │   ├── database/        # DB connections
│   │   │   ├── sentiment/       # Sentiment analysis
│   │   │   └── utils/           # Helpers
│   │   └── package.json
│   │
│   ├── scrapers/
│   │   ├── reddit/
│   │   │   ├── src/
│   │   │   │   ├── RedditScraper.ts
│   │   │   │   ├── parsers/
│   │   │   │   └── strategies/
│   │   │   └── package.json
│   │   ├── twitter/
│   │   │   ├── src/
│   │   │   │   ├── TwitterScraper.ts
│   │   │   │   ├── parsers/
│   │   │   │   └── strategies/
│   │   │   └── package.json
│   │   ├── linkedin/
│   │   │   ├── src/
│   │   │   │   ├── LinkedInScraper.ts
│   │   │   │   ├── parsers/
│   │   │   │   └── strategies/
│   │   │   └── package.json
│   │   ├── playstore/
│   │   │   ├── src/
│   │   │   │   ├── PlayStoreScraper.ts
│   │   │   │   └── parsers/
│   │   │   └── package.json
│   │   └── appstore/
│   │       ├── src/
│   │       │   ├── AppStoreScraper.ts
│   │       │   └── parsers/
│   │       └── package.json
│   │
│   ├── scheduler/
│   │   ├── src/
│   │   │   ├── index.ts
│   │   │   ├── jobs/
│   │   │   └── config/
│   │   └── package.json
│   │
│   ├── api/
│   │   ├── src/
│   │   │   ├── routes/
│   │   │   ├── middleware/
│   │   │   └── controllers/
│   │   └── package.json
│   │
│   └── dashboard/
│       ├── src/
│       │   ├── app/
│       │   ├── components/
│       │   └── lib/
│       └── package.json
│
├── config/
│   ├── proxies.json
│   ├── user-agents.json
│   └── scraper-config.yaml
│
└── scripts/
    ├── setup.sh
    ├── seed-db.sh
    └── health-check.sh
      ]]>
    </directory_structure>
  </architecture>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 2: ANTI-DETECTION & SCRAPING INFRASTRUCTURE -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <scraping_infrastructure>
    <browser_automation>
      <framework>Playwright</framework>
      <stealth_requirements>
        <requirement priority="critical">
          <name>Stealth Plugin Integration</name>
          <description>Use playwright-extra with stealth plugin to mask automation signatures</description>
          <implementation>
            <![CDATA[
import { chromium } from 'playwright-extra';
import stealth from 'puppeteer-extra-plugin-stealth';

chromium.use(stealth());

const browser = await chromium.launch({
  headless: true,
  args: [
    '--disable-blink-features=AutomationControlled',
    '--disable-dev-shm-usage',
    '--no-sandbox',
    '--disable-setuid-sandbox',
    '--disable-accelerated-2d-canvas',
    '--disable-gpu'
  ]
});
            ]]>
          </implementation>
        </requirement>
        
        <requirement priority="critical">
          <name>Navigator Property Masking</name>
          <description>Override navigator properties to appear as real browser</description>
          <properties_to_mask>
            <property>navigator.webdriver</property>
            <property>navigator.plugins</property>
            <property>navigator.languages</property>
            <property>navigator.platform</property>
          </properties_to_mask>
        </requirement>
        
        <requirement priority="high">
          <name>Canvas Fingerprint Randomization</name>
          <description>Inject noise into canvas fingerprinting to vary signatures</description>
        </requirement>
        
        <requirement priority="high">
          <name>WebGL Fingerprint Spoofing</name>
          <description>Randomize WebGL renderer and vendor strings</description>
        </requirement>
        
        <requirement priority="high">
          <name>Timezone and Locale Consistency</name>
          <description>Ensure timezone matches proxy location, consistent locale settings</description>
        </requirement>
      </stealth_requirements>
    </browser_automation>

    <proxy_management>
      <strategy>Rotating residential proxies with session persistence</strategy>
      <providers_to_consider>
        <provider>Bright Data (Luminati)</provider>
        <provider>Smartproxy</provider>
        <provider>Oxylabs</provider>
        <provider>IPRoyal</provider>
      </providers_to_consider>
      
      <implementation_spec>
        <![CDATA[
interface ProxyConfig {
  host: string;
  port: number;
  username: string;
  password: string;
  country?: string;
  sessionId?: string;
  rotationInterval: number; // minutes
}

class ProxyManager {
  private proxyPool: ProxyConfig[];
  private currentIndex: number = 0;
  private failedProxies: Map<string, number> = new Map();
  private lastRotation: Map<string, Date> = new Map();
  
  // Rotate proxy after N requests or on failure
  async getProxy(platformId: string): Promise<ProxyConfig> {
    // Implement sticky sessions for platforms requiring login
    // Rotate on 429/403 errors
    // Track and blacklist consistently failing proxies
  }
  
  async reportFailure(proxy: ProxyConfig, error: Error): Promise<void> {
    // Implement exponential backoff for failed proxies
    // Remove from pool if failure rate exceeds threshold
  }
  
  async healthCheck(): Promise<ProxyHealthReport> {
    // Periodic proxy pool health verification
  }
}
        ]]>
      </implementation_spec>
      
      <rotation_rules>
        <rule platform="twitter">Rotate every 15-25 requests (randomized)</rule>
        <rule platform="linkedin">Use sticky sessions, rotate every 30 minutes</rule>
        <rule platform="reddit">Rotate every 50-100 requests</rule>
        <rule platform="appstores">Less aggressive, rotate every 100+ requests</rule>
      </rotation_rules>
    </proxy_management>

    <human_behavior_simulation>
      <critical_importance>TRUE - This is essential to avoid detection</critical_importance>
      
      <behaviors>
        <behavior name="randomized_delays">
          <description>Variable delays between actions mimicking human reading/thinking time</description>
          <implementation>
            <![CDATA[
async function humanDelay(minMs: number, maxMs: number): Promise<void> {
  const delay = minMs + Math.random() * (maxMs - minMs);
  // Add occasional longer "thinking" pauses
  const thinkingPause = Math.random() < 0.1 ? Math.random() * 3000 : 0;
  await sleep(delay + thinkingPause);
}

async function humanType(page: Page, selector: string, text: string): Promise<void> {
  await page.click(selector);
  for (const char of text) {
    await page.type(selector, char, { delay: 50 + Math.random() * 150 });
    // Occasional typo and backspace
    if (Math.random() < 0.02) {
      await page.type(selector, 'x');
      await humanDelay(100, 300);
      await page.keyboard.press('Backspace');
    }
  }
}
            ]]>
          </implementation>
        </behavior>
        
        <behavior name="mouse_movements">
          <description>Natural mouse movement patterns with bezier curves</description>
          <implementation>
            <![CDATA[
async function humanMouseMove(page: Page, targetX: number, targetY: number): Promise<void> {
  const currentPos = await page.evaluate(() => ({ x: window.mouseX || 0, y: window.mouseY || 0 }));
  const steps = 10 + Math.floor(Math.random() * 20);
  
  // Generate bezier curve control points
  const cp1x = currentPos.x + (targetX - currentPos.x) * Math.random();
  const cp1y = currentPos.y + (targetY - currentPos.y) * Math.random();
  
  for (let i = 0; i <= steps; i++) {
    const t = i / steps;
    const x = bezierPoint(currentPos.x, cp1x, targetX, t);
    const y = bezierPoint(currentPos.y, cp1y, targetY, t);
    await page.mouse.move(x, y);
    await sleep(5 + Math.random() * 15);
  }
}
            ]]>
          </implementation>
        </behavior>
        
        <behavior name="scroll_patterns">
          <description>Natural scrolling with variable speeds and occasional scroll-backs</description>
        </behavior>
        
        <behavior name="session_warmup">
          <description>Visit homepage, browse around before targeted actions</description>
        </behavior>
        
        <behavior name="realistic_viewport">
          <description>Use common screen resolutions, not perfect 1920x1080</description>
          <common_resolutions>
            <resolution>1366x768</resolution>
            <resolution>1536x864</resolution>
            <resolution>1440x900</resolution>
            <resolution>1280x720</resolution>
          </common_resolutions>
        </behavior>
      </behaviors>
    </human_behavior_simulation>

    <rate_limiting>
      <strategy>Adaptive rate limiting based on platform responses</strategy>
      
      <default_limits>
        <platform name="reddit">
          <requests_per_minute>3-5</requests_per_minute>
          <cooldown_on_429>exponential backoff starting 60s</cooldown_on_429>
          <daily_limit>500 requests</daily_limit>
        </platform>
        
        <platform name="twitter">
          <requests_per_minute>1-2</requests_per_minute>
          <cooldown_on_429>exponential backoff starting 300s</cooldown_on_429>
          <daily_limit>200 requests</daily_limit>
          <additional_note>Most aggressive anti-bot, use extreme caution</additional_note>
        </platform>
        
        <platform name="linkedin">
          <requests_per_minute>1-2</requests_per_minute>
          <cooldown_on_429>exponential backoff starting 600s</cooldown_on_429>
          <daily_limit>150 requests</daily_limit>
          <requires_login>true</requires_login>
        </platform>
        
        <platform name="playstore">
          <requests_per_minute>10-15</requests_per_minute>
          <cooldown_on_429>exponential backoff starting 30s</cooldown_on_429>
          <daily_limit>2000 requests</daily_limit>
        </platform>
        
        <platform name="appstore">
          <requests_per_minute>10-15</requests_per_minute>
          <cooldown_on_429>exponential backoff starting 30s</cooldown_on_429>
          <daily_limit>2000 requests</daily_limit>
        </platform>
      </default_limits>
      
      <adaptive_algorithm>
        <![CDATA[
class AdaptiveRateLimiter {
  private rates: Map<string, RateState> = new Map();
  
  interface RateState {
    currentRate: number;
    successCount: number;
    failureCount: number;
    lastAdjustment: Date;
    backoffMultiplier: number;
  }
  
  async throttle(platform: string): Promise<void> {
    const state = this.rates.get(platform);
    
    // Decrease rate on failures, increase slowly on sustained success
    if (state.failureCount > 3) {
      state.currentRate *= 0.5;
      state.backoffMultiplier *= 2;
    } else if (state.successCount > 50 && state.currentRate < maxRate) {
      state.currentRate *= 1.1;
    }
    
    await this.wait(state.currentRate);
  }
  
  reportSuccess(platform: string): void { /* increment success */ }
  reportFailure(platform: string): void { /* increment failure, trigger backoff */ }
}
        ]]>
      </adaptive_algorithm>
    </rate_limiting>

    <error_handling>
      <retry_strategy>
        <max_retries>5</max_retries>
        <backoff_type>exponential with jitter</backoff_type>
        <base_delay_ms>1000</base_delay_ms>
        <max_delay_ms>300000</max_delay_ms>
      </retry_strategy>
      
      <error_categories>
        <category type="rate_limited" codes="429">
          <action>Long backoff, rotate proxy, reduce rate</action>
        </category>
        <category type="blocked" codes="403, CAPTCHA">
          <action>Rotate proxy, warm up new session, potentially solve CAPTCHA</action>
        </category>
        <category type="network" codes="TIMEOUT, ECONNRESET">
          <action>Simple retry with same proxy</action>
        </category>
        <category type="parse_error" codes="ELEMENT_NOT_FOUND">
          <action>Log for investigation, site may have changed structure</action>
        </category>
      </error_categories>
    </error_handling>
  </scraping_infrastructure>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 3: PLATFORM-SPECIFIC SCRAPERS -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <platform_scrapers>
    
    <!-- REDDIT SCRAPER -->
    <scraper platform="reddit">
      <approach>Use old.reddit.com for simpler HTML structure</approach>
      <authentication>Not required for public subreddits</authentication>
      
      <search_strategy>
        <method>Search URL with query parameters</method>
        <url_pattern>https://old.reddit.com/search?q=matiks&amp;sort=new&amp;t=all</url_pattern>
        <subreddit_specific>https://old.reddit.com/r/{subreddit}/search?q=matiks&amp;restrict_sr=on</subreddit_specific>
      </search_strategy>
      
      <data_to_extract>
        <field name="post_id" selector="div.thing" attribute="data-fullname"/>
        <field name="title" selector="a.title"/>
        <field name="author" selector="a.author"/>
        <field name="subreddit" selector="a.subreddit"/>
        <field name="timestamp" selector="time" attribute="datetime"/>
        <field name="score" selector="div.score.unvoted"/>
        <field name="num_comments" selector="a.comments"/>
        <field name="post_url" selector="a.title" attribute="href"/>
        <field name="content" selector="div.expando div.md" notes="Expand self-posts to get content"/>
      </data_to_extract>
      
      <pagination>
        <method>Click "next" button or use after= parameter</method>
        <max_pages>10</max_pages>
      </pagination>
      
      <implementation_notes>
        <note>old.reddit.com has simpler DOM, less JavaScript</note>
        <note>Include comment scraping for full context</note>
        <note>Handle removed/deleted posts gracefully</note>
      </implementation_notes>
    </scraper>

    <!-- TWITTER/X SCRAPER -->
    <scraper platform="twitter">
      <approach>Browser automation with session management - NO OFFICIAL API</approach>
      <authentication>Required - use dedicated monitoring account</authentication>
      <difficulty_level>VERY HIGH - Most aggressive anti-bot measures</difficulty_level>
      
      <critical_requirements>
        <requirement>Use authenticated session (login required)</requirement>
        <requirement>Residential proxies MANDATORY</requirement>
        <requirement>Extreme human-like behavior</requirement>
        <requirement>Very conservative rate limits</requirement>
        <requirement>Session warming before searches</requirement>
      </critical_requirements>
      
      <login_flow>
        <step>Navigate to twitter.com with warmed session</step>
        <step>Human-like typing of credentials</step>
        <step>Handle 2FA if required (store backup codes)</step>
        <step>Verify login success before proceeding</step>
        <step>Store session cookies for reuse</step>
      </login_flow>
      
      <search_strategy>
        <method>Use Twitter Advanced Search via browser</method>
        <url_pattern>https://twitter.com/search?q=matiks&amp;src=typed_query&amp;f=live</url_pattern>
        <advanced_operators>
          <operator>"matiks" - exact match</operator>
          <operator>matiks lang:en - language filter</operator>
          <operator>matiks since:2024-01-01 - date filter</operator>
        </advanced_operators>
      </search_strategy>
      
      <data_to_extract>
        <field name="tweet_id" location="article element data attribute"/>
        <field name="author_handle" selector="div[data-testid='User-Name'] a"/>
        <field name="author_name" selector="div[data-testid='User-Name'] span"/>
        <field name="content" selector="div[data-testid='tweetText']"/>
        <field name="timestamp" selector="time" attribute="datetime"/>
        <field name="reply_count" selector="div[data-testid='reply']"/>
        <field name="retweet_count" selector="div[data-testid='retweet']"/>
        <field name="like_count" selector="div[data-testid='like']"/>
        <field name="view_count" selector="analytics link or aria-label"/>
      </data_to_extract>
      
      <anti_detection_specifics>
        <measure>Warm up session by browsing timeline first</measure>
        <measure>Random engagement actions (scroll, hover)</measure>
        <measure>Variable search frequency (not predictable patterns)</measure>
        <measure>Use multiple accounts in rotation</measure>
        <measure>Mirror real user session lengths</measure>
      </anti_detection_specifics>
      
      <fallback_strategies>
        <strategy name="Nitter Instances">
          <description>Use public Nitter instances as fallback (less reliable)</description>
          <note>Many Nitter instances are down, maintain list of working ones</note>
        </strategy>
        <strategy name="RSS Bridge">
          <description>Some RSS bridge services still work</description>
        </strategy>
      </fallback_strategies>
    </scraper>

    <!-- LINKEDIN SCRAPER -->
    <scraper platform="linkedin">
      <approach>Authenticated browser automation - NO OFFICIAL API</approach>
      <authentication>REQUIRED - dedicated monitoring account essential</authentication>
      <difficulty_level>HIGH - Strict rate limits and session validation</difficulty_level>
      
      <critical_warnings>
        <warning>LinkedIn actively detects automation and bans accounts</warning>
        <warning>Use only residential proxies from account's stated location</warning>
        <warning>Maintain realistic session patterns</warning>
        <warning>Consider using multiple aged accounts in rotation</warning>
      </critical_warnings>
      
      <account_requirements>
        <requirement>Aged account (3+ months old)</requirement>
        <requirement>Complete profile with connections</requirement>
        <requirement>Activity history before scraping</requirement>
        <requirement>Consistent login location matching proxy</requirement>
      </account_requirements>
      
      <login_flow>
        <step>Navigate to linkedin.com</step>
        <step>Human-like credential entry</step>
        <step>Handle security challenges if presented</step>
        <step>Store and reuse session cookies</step>
        <step>Session validity: 24-48 hours max</step>
      </login_flow>
      
      <search_types>
        <type name="content_search">
          <url>https://www.linkedin.com/search/results/content/?keywords=matiks</url>
          <data>Posts mentioning Matiks</data>
        </type>
        <type name="company_posts">
          <url>https://www.linkedin.com/company/matiks/posts/</url>
          <data>Official Matiks posts and engagement</data>
        </type>
      </search_types>
      
      <data_to_extract>
        <field name="post_id" location="data-urn attribute"/>
        <field name="author_name" selector="span.feed-shared-actor__name"/>
        <field name="author_title" selector="span.feed-shared-actor__description"/>
        <field name="author_profile_url" selector="a.feed-shared-actor__container-link"/>
        <field name="content" selector="div.feed-shared-update-v2__description"/>
        <field name="timestamp" selector="span.feed-shared-actor__sub-description"/>
        <field name="reaction_count" selector="span.social-details-social-counts__reactions-count"/>
        <field name="comment_count" selector="button.social-details-social-counts__comments"/>
        <field name="share_count" selector="button.social-details-social-counts__item--with-social-proof"/>
      </data_to_extract>
      
      <session_management>
        <strategy>Sticky sessions with same proxy for duration</strategy>
        <max_session_length>2 hours of activity</max_session_length>
        <cooldown_between_sessions>4-8 hours</cooldown_between_sessions>
      </session_management>
    </scraper>

    <!-- GOOGLE PLAY STORE SCRAPER -->
    <scraper platform="playstore">
      <approach>Direct HTTP requests or lightweight browser automation</approach>
      <authentication>Not required</authentication>
      <difficulty_level>MEDIUM - More lenient than social platforms</difficulty_level>
      
      <app_identification>
        <method>Search for Matiks app or use direct package ID</method>
        <search_url>https://play.google.com/store/search?q=matiks&amp;c=apps</search_url>
        <direct_url>https://play.google.com/store/apps/details?id={package_id}</direct_url>
      </app_identification>
      
      <reviews_extraction>
        <url_pattern>https://play.google.com/store/apps/details?id={package_id}&amp;showAllReviews=true</url_pattern>
        
        <data_to_extract>
          <field name="review_id" selector="div.RHo1pe" attribute="data-review-id"/>
          <field name="author_name" selector="span.X43Kjb"/>
          <field name="author_avatar" selector="img.T75of" attribute="src"/>
          <field name="rating" selector="div.iXRFPc" attribute="aria-label"/>
          <field name="review_date" selector="span.bp9Aid"/>
          <field name="review_text" selector="span.QLtgib"/>
          <field name="helpful_count" selector="div.AJTPZc"/>
          <field name="app_version" selector="span.nt2C1d span" notes="Version reviewed on"/>
          <field name="developer_reply" selector="div.LVQB0b"/>
          <field name="developer_reply_date" selector="span.I9Jtec"/>
        </data_to_extract>
        
        <pagination>
          <method>Scroll to load more reviews (infinite scroll)</method>
          <alternative>Use internal API endpoints if discoverable</alternative>
          <max_reviews_per_run>500</max_reviews_per_run>
        </pagination>
      </reviews_extraction>
      
      <library_option>
        <name>google-play-scraper (npm)</name>
        <note>Consider using existing library as base, with custom anti-detection layer</note>
      </library_option>
    </scraper>

    <!-- APPLE APP STORE SCRAPER -->
    <scraper platform="appstore">
      <approach>RSS feeds + web scraping hybrid</approach>
      <authentication>Not required</authentication>
      <difficulty_level>MEDIUM</difficulty_level>
      
      <app_identification>
        <search_url>https://www.apple.com/search/matiks?src=globalnav</search_url>
        <direct_url>https://apps.apple.com/app/id{app_id}</direct_url>
      </app_identification>
      
      <reviews_extraction>
        <primary_method>
          <name>Customer Reviews RSS Feed</name>
          <url_pattern>https://itunes.apple.com/rss/customerreviews/id={app_id}/sortBy=mostRecent/json</url_pattern>
          <note>Official RSS feed, most reliable method</note>
        </primary_method>
        
        <secondary_method>
          <name>Web Scraping</name>
          <url>https://apps.apple.com/app/id{app_id}#see-all/reviews</url>
          <note>For additional data not in RSS</note>
        </secondary_method>
        
        <data_to_extract>
          <field name="review_id" path="entry.id.label"/>
          <field name="author_name" path="entry.author.name.label"/>
          <field name="author_uri" path="entry.author.uri.label"/>
          <field name="rating" path="entry.im:rating.label"/>
          <field name="review_title" path="entry.title.label"/>
          <field name="review_text" path="entry.content.label"/>
          <field name="app_version" path="entry.im:version.label"/>
          <field name="review_date" path="entry.updated.label"/>
        </data_to_extract>
        
        <pagination>
          <method>RSS feed pagination via page parameter</method>
          <url_pattern>https://itunes.apple.com/rss/customerreviews/page={n}/id={app_id}/sortBy=mostRecent/json</url_pattern>
          <max_pages>10</max_pages>
        </pagination>
      </reviews_extraction>
      
      <library_option>
        <name>app-store-scraper (npm)</name>
        <note>Solid foundation, customize as needed</note>
      </library_option>
    </scraper>
  </platform_scrapers>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 4: DATA PIPELINE & STORAGE -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <data_pipeline>
    <stages>
      <stage name="Collection">
        <description>Raw data collected by scrapers</description>
        <output>Raw JSON documents with platform metadata</output>
      </stage>
      
      <stage name="Normalization">
        <description>Transform platform-specific formats to unified schema</description>
        <operations>
          <operation>Standardize date formats to ISO 8601</operation>
          <operation>Normalize engagement metrics naming</operation>
          <operation>Extract and clean text content</operation>
          <operation>Generate unique content hash for deduplication</operation>
        </operations>
      </stage>
      
      <stage name="Enrichment">
        <description>Add derived data and analysis</description>
        <operations>
          <operation>Sentiment analysis scoring</operation>
          <operation>Language detection</operation>
          <operation>Keyword/topic extraction</operation>
          <operation>Author influence scoring (followers, engagement history)</operation>
        </operations>
      </stage>
      
      <stage name="Storage">
        <description>Persist to database with proper indexing</description>
      </stage>
    </stages>

    <sentiment_analysis>
      <approach>Local ML model, no external API dependency</approach>
      
      <options>
        <option name="VADER" language="Python">
          <description>Rule-based, good for social media text</description>
          <pros>Fast, no training required, handles emoji/slang</pros>
          <cons>Less accurate than ML models</cons>
        </option>
        
        <option name="Transformers" language="Python">
          <description>BERT-based sentiment classification</description>
          <model>nlptown/bert-base-multilingual-uncased-sentiment</model>
          <model>cardiffnlp/twitter-roberta-base-sentiment-latest</model>
          <pros>High accuracy, handles nuance</pros>
          <cons>Slower, requires more resources</cons>
        </option>
        
        <option name="Hybrid" recommended="true">
          <description>VADER for quick scoring, transformers for detailed analysis</description>
        </option>
      </options>
      
      <output_schema>
        <field name="sentiment_score" type="float" range="-1 to 1"/>
        <field name="sentiment_label" type="enum" values="positive,neutral,negative"/>
        <field name="confidence" type="float" range="0 to 1"/>
        <field name="emotion_breakdown" type="object" optional="true"/>
      </output_schema>
    </sentiment_analysis>
  </data_pipeline>

  <database>
    <technology>PostgreSQL with TimescaleDB extension</technology>
    
    <schema>
      <![CDATA[
-- Core tables
CREATE TABLE platforms (
  id SERIAL PRIMARY KEY,
  name VARCHAR(50) UNIQUE NOT NULL,
  type VARCHAR(20) NOT NULL, -- 'social' or 'appstore'
  config JSONB
);

CREATE TABLE mentions (
  id BIGSERIAL PRIMARY KEY,
  platform_id INTEGER REFERENCES platforms(id),
  external_id VARCHAR(255) NOT NULL,
  content_hash VARCHAR(64) NOT NULL,
  author_name VARCHAR(255),
  author_handle VARCHAR(255),
  author_url TEXT,
  content TEXT,
  url TEXT,
  created_at TIMESTAMPTZ NOT NULL,
  scraped_at TIMESTAMPTZ DEFAULT NOW(),
  engagement JSONB, -- likes, shares, comments, etc.
  sentiment_score DECIMAL(4,3),
  sentiment_label VARCHAR(20),
  raw_data JSONB,
  UNIQUE(platform_id, external_id)
);

CREATE TABLE reviews (
  id BIGSERIAL PRIMARY KEY,
  platform_id INTEGER REFERENCES platforms(id),
  external_id VARCHAR(255) NOT NULL,
  content_hash VARCHAR(64) NOT NULL,
  author_name VARCHAR(255),
  rating INTEGER CHECK (rating >= 1 AND rating <= 5),
  title TEXT,
  content TEXT,
  app_version VARCHAR(50),
  review_date TIMESTAMPTZ NOT NULL,
  scraped_at TIMESTAMPTZ DEFAULT NOW(),
  helpful_count INTEGER DEFAULT 0,
  developer_reply TEXT,
  developer_reply_date TIMESTAMPTZ,
  sentiment_score DECIMAL(4,3),
  sentiment_label VARCHAR(20),
  raw_data JSONB,
  UNIQUE(platform_id, external_id)
);

CREATE TABLE scrape_jobs (
  id BIGSERIAL PRIMARY KEY,
  platform_id INTEGER REFERENCES platforms(id),
  job_type VARCHAR(50),
  status VARCHAR(20) DEFAULT 'pending',
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  items_found INTEGER DEFAULT 0,
  items_new INTEGER DEFAULT 0,
  errors JSONB,
  metadata JSONB
);

-- Indexes for performance
CREATE INDEX idx_mentions_platform_created ON mentions(platform_id, created_at DESC);
CREATE INDEX idx_mentions_sentiment ON mentions(sentiment_label);
CREATE INDEX idx_mentions_content_search ON mentions USING gin(to_tsvector('english', content));

CREATE INDEX idx_reviews_platform_date ON reviews(platform_id, review_date DESC);
CREATE INDEX idx_reviews_rating ON reviews(rating);
CREATE INDEX idx_reviews_content_search ON reviews USING gin(to_tsvector('english', content));

-- TimescaleDB hypertables for time-series optimization
SELECT create_hypertable('mentions', 'created_at', if_not_exists => TRUE);
SELECT create_hypertable('reviews', 'review_date', if_not_exists => TRUE);
      ]]>
    </schema>
  </database>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 5: SCHEDULER & AUTOMATION -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <scheduler>
    <technology>Bull (Node.js job queue) with Redis backend</technology>
    
    <job_definitions>
      <job name="reddit_scrape">
        <cron>0 */4 * * *</cron>
        <description>Every 4 hours</description>
        <timeout>30 minutes</timeout>
        <retries>3</retries>
      </job>
      
      <job name="twitter_scrape">
        <cron>0 */6 * * *</cron>
        <description>Every 6 hours (conservative due to anti-bot)</description>
        <timeout>45 minutes</timeout>
        <retries>2</retries>
      </job>
      
      <job name="linkedin_scrape">
        <cron>0 8,20 * * *</cron>
        <description>Twice daily during business hours</description>
        <timeout>30 minutes</timeout>
        <retries>2</retries>
      </job>
      
      <job name="playstore_reviews">
        <cron>0 */2 * * *</cron>
        <description>Every 2 hours</description>
        <timeout>15 minutes</timeout>
        <retries>3</retries>
      </job>
      
      <job name="appstore_reviews">
        <cron>30 */2 * * *</cron>
        <description>Every 2 hours, offset from Play Store</description>
        <timeout>15 minutes</timeout>
        <retries>3</retries>
      </job>
      
      <job name="spreadsheet_sync">
        <cron>0 * * * *</cron>
        <description>Hourly export to Google Sheets</description>
        <timeout>10 minutes</timeout>
        <retries>3</retries>
      </job>
      
      <job name="health_check">
        <cron>*/15 * * * *</cron>
        <description>Every 15 minutes - system health verification</description>
        <timeout>5 minutes</timeout>
      </job>
    </job_definitions>
    
    <implementation>
      <![CDATA[
import Queue from 'bull';
import { redis } from './config';

const scraperQueue = new Queue('scraper-jobs', { redis });

// Job processor with error handling and logging
scraperQueue.process('reddit_scrape', async (job) => {
  const logger = createJobLogger(job.id);
  try {
    logger.info('Starting Reddit scrape');
    const result = await redditScraper.run(job.data);
    logger.info(`Completed: ${result.newItems} new items`);
    return result;
  } catch (error) {
    logger.error('Failed', error);
    throw error; // Trigger retry
  }
});

// Retry logic with exponential backoff
scraperQueue.on('failed', (job, error) => {
  const delay = Math.pow(2, job.attemptsMade) * 60000; // 1min, 2min, 4min...
  job.retry({ delay });
});

// Dead letter queue for persistent failures
scraperQueue.on('failed', (job, error) => {
  if (job.attemptsMade >= job.opts.attempts) {
    deadLetterQueue.add(job.data, { originalError: error });
    alerting.send(`Job ${job.name} failed permanently after ${job.attemptsMade} attempts`);
  }
});
      ]]>
    </implementation>
  </scheduler>

  <logging_monitoring>
    <logging>
      <technology>Winston with structured JSON logging</technology>
      <log_levels>error, warn, info, debug</log_levels>
      <storage>File rotation + optional cloud logging (Logtail/Datadog)</storage>
      
      <required_log_events>
        <event>Job start/complete/fail</event>
        <event>Items scraped count</event>
        <event>Rate limit encounters</event>
        <event>Proxy rotations</event>
        <event>Error details with stack traces</event>
        <event>Session status changes</event>
      </required_log_events>
    </logging>
    
    <health_dashboard>
      <metrics>
        <metric>Total items by platform (last 24h, 7d, 30d)</metric>
        <metric>Scrape success rate by platform</metric>
        <metric>Average items per scrape</metric>
        <metric>Error rate and types</metric>
        <metric>Proxy health status</metric>
        <metric>Queue depth and processing time</metric>
      </metrics>
    </health_dashboard>
    
    <alerting>
      <channels>Email, Slack webhook, Discord webhook</channels>
      <triggers>
        <trigger>Scraper failure rate exceeds 50%</trigger>
        <trigger>No new items in 24 hours</trigger>
        <trigger>Account blocked/suspended</trigger>
        <trigger>Proxy pool exhausted</trigger>
      </triggers>
    </alerting>
  </logging_monitoring>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 6: WEB DASHBOARD -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <dashboard>
    <technology>Next.js 14 with App Router, Tailwind CSS, Prisma ORM</technology>
    
    <pages>
      <page path="/">
        <name>Overview Dashboard</name>
        <components>
          <component>Summary stats cards (total mentions, avg sentiment, etc.)</component>
          <component>Trend chart (mentions over time)</component>
          <component>Sentiment distribution pie chart</component>
          <component>Recent activity feed</component>
          <component>Platform health status</component>
        </components>
      </page>
      
      <page path="/mentions">
        <name>Social Media Mentions</name>
        <components>
          <component>Filterable data table</component>
          <component>Platform filter (Reddit, Twitter, LinkedIn)</component>
          <component>Date range picker</component>
          <component>Sentiment filter</component>
          <component>Full-text search</component>
          <component>Export to CSV button</component>
        </components>
        <features>
          <feature>Infinite scroll or pagination</feature>
          <feature>Click to view original post</feature>
          <feature>Engagement metrics display</feature>
        </features>
      </page>
      
      <page path="/reviews">
        <name>App Store Reviews</name>
        <components>
          <component>Filterable data table</component>
          <component>Store filter (Play Store, App Store)</component>
          <component>Rating filter (1-5 stars)</component>
          <component>Date range picker</component>
          <component>Version filter</component>
          <component>Full-text search</component>
          <component>Rating distribution chart</component>
        </components>
      </page>
      
      <page path="/analytics">
        <name>Analytics &amp; Insights</name>
        <components>
          <component>Sentiment trend over time</component>
          <component>Platform comparison</component>
          <component>Word cloud of common terms</component>
          <component>Rating trends by version</component>
          <component>Top positive/negative mentions</component>
        </components>
      </page>
      
      <page path="/settings">
        <name>System Settings</name>
        <components>
          <component>Scrape schedule configuration</component>
          <component>Proxy management</component>
          <component>Alert configuration</component>
          <component>Export settings (Google Sheets connection)</component>
        </components>
      </page>
      
      <page path="/logs">
        <name>System Logs</name>
        <components>
          <component>Real-time log viewer</component>
          <component>Job history table</component>
          <component>Error browser</component>
        </components>
      </page>
    </pages>
    
    <ui_requirements>
      <requirement>Dark mode support</requirement>
      <requirement>Responsive design (mobile-friendly)</requirement>
      <requirement>Real-time updates via WebSocket or SSE</requirement>
      <requirement>Loading states and error handling</requirement>
      <requirement>Accessible (WCAG 2.1 AA)</requirement>
    </ui_requirements>
  </dashboard>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 7: EXPORT & INTEGRATION -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <export_integration>
    <google_sheets>
      <description>Automated sync to Google Sheets for easy stakeholder access</description>
      
      <implementation>
        <authentication>Service account with Sheets API access</authentication>
        <library>googleapis npm package</library>
        
        <sheets_structure>
          <sheet name="Social Mentions">
            <columns>Date, Platform, Author, Content, URL, Likes, Comments, Shares, Sentiment, Score</columns>
          </sheet>
          <sheet name="App Reviews">
            <columns>Date, Store, Author, Rating, Title, Review, Version, Helpful, Sentiment</columns>
          </sheet>
          <sheet name="Summary Stats">
            <columns>Date, Total Mentions, Avg Sentiment, Positive %, Neutral %, Negative %, Avg Rating</columns>
          </sheet>
        </sheets_structure>
        
        <sync_behavior>
          <mode>Append new rows, update daily summary</mode>
          <frequency>Hourly</frequency>
          <deduplication>By unique ID field</deduplication>
        </sync_behavior>
      </implementation>
    </google_sheets>
    
    <api_endpoints>
      <endpoint method="GET" path="/api/mentions">
        <params>platform, startDate, endDate, sentiment, search, page, limit</params>
        <response>Paginated mentions array</response>
      </endpoint>
      <endpoint method="GET" path="/api/reviews">
        <params>store, startDate, endDate, minRating, maxRating, search, page, limit</params>
        <response>Paginated reviews array</response>
      </endpoint>
      <endpoint method="GET" path="/api/stats">
        <params>startDate, endDate</params>
        <response>Aggregated statistics object</response>
      </endpoint>
      <endpoint method="GET" path="/api/export">
        <params>type (mentions/reviews), format (csv/json), filters...</params>
        <response>File download</response>
      </endpoint>
    </api_endpoints>
  </export_integration>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 8: DEPLOYMENT -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <deployment>
    <containerization>
      <dockerfile_example>
        <![CDATA[
# Base image with Playwright dependencies
FROM mcr.microsoft.com/playwright:v1.40.0-jammy

WORKDIR /app

# Install Node.js dependencies
COPY package*.json ./
RUN npm ci --only=production

# Copy source
COPY . .

# Build
RUN npm run build

# Run
CMD ["npm", "start"]
        ]]>
      </dockerfile_example>
      
      <docker_compose>
        <![CDATA[
version: '3.8'

services:
  postgres:
    image: timescale/timescaledb:latest-pg15
    environment:
      POSTGRES_DB: matiks_monitor
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"

  scraper:
    build:
      context: .
      dockerfile: Dockerfile.scraper
    depends_on:
      - postgres
      - redis
    environment:
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/matiks_monitor
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./config:/app/config

  scheduler:
    build:
      context: .
      dockerfile: Dockerfile.scheduler
    depends_on:
      - postgres
      - redis
      - scraper
    environment:
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/matiks_monitor
      - REDIS_URL=redis://redis:6379

  dashboard:
    build:
      context: ./packages/dashboard
      dockerfile: Dockerfile
    depends_on:
      - postgres
    environment:
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/matiks_monitor
    ports:
      - "3000:3000"

volumes:
  postgres_data:
  redis_data:
        ]]>
      </docker_compose>
    </containerization>
    
    <hosting_options>
      <option name="VPS" recommended="true">
        <providers>DigitalOcean, Hetzner, Linode, Vultr</providers>
        <min_specs>4GB RAM, 2 vCPU, 50GB SSD</min_specs>
        <estimated_cost>$20-40/month</estimated_cost>
      </option>
      <option name="Cloud">
        <providers>AWS (ECS/EC2), GCP (Cloud Run), Azure</providers>
        <estimated_cost>$50-100/month</estimated_cost>
      </option>
    </hosting_options>
  </deployment>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 9: CONFIGURATION -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <configuration>
    <env_variables>
      <![CDATA[
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/matiks_monitor

# Redis
REDIS_URL=redis://localhost:6379

# Proxy Configuration
PROXY_PROVIDER=brightdata  # or smartproxy, oxylabs
PROXY_HOST=brd.superproxy.io
PROXY_PORT=22225
PROXY_USERNAME=your_username
PROXY_PASSWORD=your_password

# Platform Credentials (encrypted at rest)
TWITTER_USERNAME=your_monitor_account
TWITTER_PASSWORD=encrypted_password
TWITTER_2FA_SECRET=optional_totp_secret

LINKEDIN_EMAIL=your_monitor_account@email.com
LINKEDIN_PASSWORD=encrypted_password

# App Store IDs
PLAYSTORE_APP_ID=com.matiks.app
APPSTORE_APP_ID=123456789

# Google Sheets Export
GOOGLE_SERVICE_ACCOUNT_KEY=path/to/service-account.json
GOOGLE_SPREADSHEET_ID=your_spreadsheet_id

# Alerting
SLACK_WEBHOOK_URL=https://hooks.slack.com/...
ALERT_EMAIL=alerts@yourcompany.com

# Dashboard
DASHBOARD_SECRET=random_secret_for_sessions
DASHBOARD_URL=https://monitor.yourcompany.com
      ]]>
    </env_variables>
    
    <scraper_config_yaml>
      <![CDATA[
# config/scraper-config.yaml

global:
  userAgentRotation: true
  proxyRotation: true
  maxConcurrentBrowsers: 3
  screenshotOnError: true

platforms:
  reddit:
    enabled: true
    searchTerms:
      - "matiks"
      - "matiks app"
    subreddits:
      - "all"
      - "fintech"
      - "personalfinance"
    maxPagesPerRun: 5
    rateLimit:
      requestsPerMinute: 4
      
  twitter:
    enabled: true
    searchTerms:
      - "matiks"
      - "@matiks"
      - "#matiks"
    rateLimit:
      requestsPerMinute: 1.5
    sessionWarmup:
      enabled: true
      durationSeconds: 30
      
  linkedin:
    enabled: true
    searchTerms:
      - "matiks"
    companyPages:
      - "matiks"
    rateLimit:
      requestsPerMinute: 1
      
  playstore:
    enabled: true
    appId: "com.matiks.app"
    maxReviewsPerRun: 200
    rateLimit:
      requestsPerMinute: 12
      
  appstore:
    enabled: true
    appId: "123456789"
    countries:
      - "us"
      - "gb"
      - "in"
    maxReviewsPerRun: 200
    rateLimit:
      requestsPerMinute: 12
      ]]>
    </scraper_config_yaml>
  </configuration>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 10: DELIVERABLES CHECKLIST -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <deliverables>
    <checklist>
      <item category="Source Code">
        <task>Complete monorepo with all packages</task>
        <task>Core library with browser pool, proxy management, anti-detection</task>
        <task>Reddit scraper with search and comment extraction</task>
        <task>Twitter scraper with login flow and session management</task>
        <task>LinkedIn scraper with authenticated access</task>
        <task>Play Store review scraper</task>
        <task>App Store review scraper (RSS + web)</task>
        <task>Scheduler with all job definitions</task>
        <task>Data pipeline with sentiment analysis</task>
        <task>REST API for data access</task>
        <task>Next.js dashboard with all pages</task>
        <task>Google Sheets export integration</task>
      </item>
      
      <item category="Deployment">
        <task>Dockerfiles for all services</task>
        <task>Docker Compose for local development</task>
        <task>Production Docker Compose</task>
        <task>Environment configuration templates</task>
        <task>Database migration scripts</task>
      </item>
      
      <item category="Documentation">
        <task>README with quick start guide</task>
        <task>Architecture documentation</task>
        <task>Setup and deployment guide</task>
        <task>Configuration reference</task>
        <task>Troubleshooting guide</task>
        <task>API documentation</task>
      </item>
      
      <item category="Demo">
        <task>Video walkthrough of system setup</task>
        <task>Dashboard tour demonstration</task>
        <task>Live data collection demo</task>
      </item>
    </checklist>
  </deliverables>

  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  <!-- SECTION 11: RISK MITIGATION -->
  <!-- ═══════════════════════════════════════════════════════════════════════════ -->
  
  <risk_mitigation>
    <risk name="Account Bans">
      <likelihood>High for Twitter/LinkedIn</likelihood>
      <mitigation>
        <strategy>Use multiple aged accounts in rotation</strategy>
        <strategy>Implement ultra-conservative rate limits</strategy>
        <strategy>Perfect human behavior simulation</strategy>
        <strategy>Have backup accounts ready</strategy>
      </mitigation>
    </risk>
    
    <risk name="IP Blocking">
      <likelihood>Medium</likelihood>
      <mitigation>
        <strategy>Residential proxy pool (not datacenter)</strategy>
        <strategy>Automatic proxy rotation on block detection</strategy>
        <strategy>Geographic distribution matching account locations</strategy>
      </mitigation>
    </risk>
    
    <risk name="DOM Structure Changes">
      <likelihood>Medium</likelihood>
      <mitigation>
        <strategy>Robust selector fallbacks</strategy>
        <strategy>Regular automated structure validation</strategy>
        <strategy>Alert on parse failures for quick fixes</strategy>
        <strategy>Modular parser design for easy updates</strategy>
      </mitigation>
    </risk>
    
    <risk name="CAPTCHA Challenges">
      <likelihood>Medium for Twitter</likelihood>
      <mitigation>
        <strategy>Integrate CAPTCHA solving service (2Captcha, Anti-Captcha)</strategy>
        <strategy>Reduce scrape frequency to avoid triggers</strategy>
        <strategy>Session warming to build trust</strategy>
      </mitigation>
    </risk>
  </risk_mitigation>
</project_specification>
